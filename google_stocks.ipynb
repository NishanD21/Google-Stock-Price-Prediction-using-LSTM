{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a41dcd",
   "metadata": {},
   "source": [
    "### Google Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa34bb5",
   "metadata": {},
   "source": [
    "#### 1. Fire the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d86602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab895372",
   "metadata": {},
   "source": [
    "#### 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9000a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>49.813286</td>\n",
       "      <td>51.835709</td>\n",
       "      <td>47.800831</td>\n",
       "      <td>49.982655</td>\n",
       "      <td>49.982655</td>\n",
       "      <td>44871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>50.316402</td>\n",
       "      <td>54.336334</td>\n",
       "      <td>50.062355</td>\n",
       "      <td>53.952770</td>\n",
       "      <td>53.952770</td>\n",
       "      <td>22942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>55.168217</td>\n",
       "      <td>56.528118</td>\n",
       "      <td>54.321388</td>\n",
       "      <td>54.495735</td>\n",
       "      <td>54.495735</td>\n",
       "      <td>18342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>55.412300</td>\n",
       "      <td>55.591629</td>\n",
       "      <td>51.591621</td>\n",
       "      <td>52.239193</td>\n",
       "      <td>52.239193</td>\n",
       "      <td>15319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>52.284027</td>\n",
       "      <td>53.798351</td>\n",
       "      <td>51.746044</td>\n",
       "      <td>52.802086</td>\n",
       "      <td>52.802086</td>\n",
       "      <td>9232100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2004-08-19  49.813286  51.835709  47.800831  49.982655  49.982655  44871300\n",
       "1  2004-08-20  50.316402  54.336334  50.062355  53.952770  53.952770  22942800\n",
       "2  2004-08-23  55.168217  56.528118  54.321388  54.495735  54.495735  18342800\n",
       "3  2004-08-24  55.412300  55.591629  51.591621  52.239193  52.239193  15319700\n",
       "4  2004-08-25  52.284027  53.798351  51.746044  52.802086  52.802086   9232100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing train data\n",
    "\n",
    "dataset_train = pd.read_csv('GOOG.csv')\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb85012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4006, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b3deeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2530320c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8582b4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4006, 7)\n",
      "All Timestamps: 4006\n",
      "Selected Features: ['Open', 'High', 'Low', 'Close', 'Adj Close']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_5548\\3968157361.py:8: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dataset_train['Date'] = pd.to_datetime(dataset_train['Date'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# selecting features for training and predictions\n",
    "\n",
    "cols = list(dataset_train)[1:6]\n",
    "\n",
    "# extract dates (will be used in visualization)\n",
    "\n",
    "datelist_train = list(dataset_train['Date'])\n",
    "dataset_train['Date'] = pd.to_datetime(dataset_train['Date'], dayfirst=True)\n",
    "\n",
    "print(f'Training set shape: {dataset_train.shape}')\n",
    "print(f'All Timestamps: {len(datelist_train)}')\n",
    "print(f'Selected Features: {cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e5c74",
   "metadata": {},
   "source": [
    "#### 3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6888109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set : (4006, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  49.813286,   51.835709,   47.800831,   49.982655,   49.982655],\n",
       "       [  50.316402,   54.336334,   50.062355,   53.95277 ,   53.95277 ],\n",
       "       [  55.168217,   56.528118,   54.321388,   54.495735,   54.495735],\n",
       "       ...,\n",
       "       [1523.130005, 1535.329956, 1498.      , 1513.640015, 1513.640015],\n",
       "       [1500.      , 1518.689941, 1486.310059, 1518.      , 1518.      ],\n",
       "       [1521.619995, 1523.439941, 1498.420044, 1515.550049, 1515.550049]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing all the commas and converting data to matrix shape format\n",
    "\n",
    "dataset_train = dataset_train[cols].astype(str)\n",
    "\n",
    "for i in cols:\n",
    "    for j in range(0, len(dataset_train)):\n",
    "        dataset_train[i][j] = dataset_train[i][j].replace(',','')\n",
    "\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# using multiple features (predictors)\n",
    "training_set = dataset_train.to_numpy()\n",
    "\n",
    "print(f'Shape of training set : {training_set.shape}')\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe37fd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.27195197],\n",
       "       [-1.27058974],\n",
       "       [-1.25745309],\n",
       "       ...,\n",
       "       [ 2.71716347],\n",
       "       [ 2.65453724],\n",
       "       [ 2.713075  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:,0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78fe8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3857, 90, 4)\n",
      "y_train shape: (3857, 1)\n"
     ]
    }
   ],
   "source": [
    "# creating a data structure with 90 timestamps and 1 output\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "n_past = 90 #number of past days we need to predict the future\n",
    "n_future = 60 #number of days we need to predict into the future\n",
    "\n",
    "for i in range(n_past,len(training_set_scaled) - n_future + 1):\n",
    "    x_train.append(training_set_scaled[i-n_past:i,0:dataset_train.shape[1]-1])\n",
    "    y_train.append(training_set_scaled[i+n_future-1:i+n_future,0])\n",
    "\n",
    "x_train,y_train = np.array(x_train),np.array(y_train)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac77cc",
   "metadata": {},
   "source": [
    "#### Create a model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe4d4",
   "metadata": {},
   "source": [
    "##### Building the LSTM based Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0acfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages from keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacb86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model building\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 64,\n",
    "               return_sequences=True,\n",
    "               input_shape = (n_past,dataset_train.shape[1]-1)))\n",
    "model.add(LSTM(units = 10,\n",
    "          return_sequences = False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1,\n",
    "                activation = 'linear'))\n",
    "model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "              loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb88965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.1225\n",
      "Epoch 1: val_loss improved from inf to 0.87081, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 396ms/step - loss: 0.1191 - val_loss: 0.8708 - learning_rate: 0.0100\n",
      "Epoch 2/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 0.0376\n",
      "Epoch 2: val_loss improved from 0.87081 to 0.40711, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - loss: 0.0375 - val_loss: 0.4071 - learning_rate: 0.0100\n",
      "Epoch 3/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.0315\n",
      "Epoch 3: val_loss did not improve from 0.40711\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 0.0316 - val_loss: 0.5594 - learning_rate: 0.0100\n",
      "Epoch 4/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.0394\n",
      "Epoch 4: val_loss did not improve from 0.40711\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 271ms/step - loss: 0.0393 - val_loss: 0.6478 - learning_rate: 0.0100\n",
      "Epoch 5/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 0.0320\n",
      "Epoch 5: val_loss did not improve from 0.40711\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 282ms/step - loss: 0.0319 - val_loss: 0.5570 - learning_rate: 0.0100\n",
      "Epoch 6/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0308\n",
      "Epoch 6: val_loss did not improve from 0.40711\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - loss: 0.0307 - val_loss: 0.4753 - learning_rate: 0.0100\n",
      "Epoch 7/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - loss: 0.0276\n",
      "Epoch 7: val_loss improved from 0.40711 to 0.25513, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 0.0276 - val_loss: 0.2551 - learning_rate: 0.0100\n",
      "Epoch 8/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.0289\n",
      "Epoch 8: val_loss improved from 0.25513 to 0.24917, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 288ms/step - loss: 0.0288 - val_loss: 0.2492 - learning_rate: 0.0100\n",
      "Epoch 9/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - loss: 0.0284\n",
      "Epoch 9: val_loss did not improve from 0.24917\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 266ms/step - loss: 0.0284 - val_loss: 0.3208 - learning_rate: 0.0100\n",
      "Epoch 10/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 0.0277\n",
      "Epoch 10: val_loss improved from 0.24917 to 0.19303, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 267ms/step - loss: 0.0277 - val_loss: 0.1930 - learning_rate: 0.0100\n",
      "Epoch 11/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 0.0269\n",
      "Epoch 11: val_loss improved from 0.19303 to 0.16338, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 275ms/step - loss: 0.0269 - val_loss: 0.1634 - learning_rate: 0.0100\n",
      "Epoch 12/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 0.0273\n",
      "Epoch 12: val_loss did not improve from 0.16338\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 263ms/step - loss: 0.0273 - val_loss: 0.1793 - learning_rate: 0.0100\n",
      "Epoch 13/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - loss: 0.0262\n",
      "Epoch 13: val_loss improved from 0.16338 to 0.13730, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 0.0261 - val_loss: 0.1373 - learning_rate: 0.0100\n",
      "Epoch 14/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - loss: 0.0261\n",
      "Epoch 14: val_loss did not improve from 0.13730\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 268ms/step - loss: 0.0261 - val_loss: 0.1489 - learning_rate: 0.0100\n",
      "Epoch 15/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 0.0300\n",
      "Epoch 15: val_loss improved from 0.13730 to 0.10482, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 272ms/step - loss: 0.0299 - val_loss: 0.1048 - learning_rate: 0.0100\n",
      "Epoch 16/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 0.0293\n",
      "Epoch 16: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 269ms/step - loss: 0.0292 - val_loss: 0.1528 - learning_rate: 0.0100\n",
      "Epoch 17/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.0258\n",
      "Epoch 17: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 0.0258 - val_loss: 0.1467 - learning_rate: 0.0100\n",
      "Epoch 18/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - loss: 0.0263\n",
      "Epoch 18: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 259ms/step - loss: 0.0263 - val_loss: 0.1660 - learning_rate: 0.0100\n",
      "Epoch 19/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0246\n",
      "Epoch 19: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 255ms/step - loss: 0.0246 - val_loss: 0.1376 - learning_rate: 0.0100\n",
      "Epoch 20/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 0.0252\n",
      "Epoch 20: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 277ms/step - loss: 0.0252 - val_loss: 0.1955 - learning_rate: 0.0100\n",
      "Epoch 21/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0259\n",
      "Epoch 21: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - loss: 0.0259 - val_loss: 0.1309 - learning_rate: 0.0100\n",
      "Epoch 22/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0275\n",
      "Epoch 22: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 255ms/step - loss: 0.0275 - val_loss: 0.1447 - learning_rate: 0.0100\n",
      "Epoch 23/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0269\n",
      "Epoch 23: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 253ms/step - loss: 0.0268 - val_loss: 0.1305 - learning_rate: 0.0100\n",
      "Epoch 24/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 0.0260\n",
      "Epoch 24: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - loss: 0.0261 - val_loss: 0.1605 - learning_rate: 0.0100\n",
      "Epoch 25/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 0.0241\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10482\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 270ms/step - loss: 0.0241 - val_loss: 0.1326 - learning_rate: 0.0100\n",
      "Epoch 25: early stopping\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss',min_delta=1e-10,patience=10,verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=10,verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.weights.h5',monitor='val_loss',verbose=1,\n",
    "save_best_only=True,save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(x_train,y_train,shuffle=True,epochs = 30,callbacks=[es,rlr,mcp,tb],\n",
    "validation_split=0.2,verbose =1, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
