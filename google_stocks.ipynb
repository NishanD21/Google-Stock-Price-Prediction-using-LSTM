{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a41dcd",
   "metadata": {},
   "source": [
    "### Google Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa34bb5",
   "metadata": {},
   "source": [
    "#### 1. Fire the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d86602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab895372",
   "metadata": {},
   "source": [
    "#### 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9000a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>49.813286</td>\n",
       "      <td>51.835709</td>\n",
       "      <td>47.800831</td>\n",
       "      <td>49.982655</td>\n",
       "      <td>49.982655</td>\n",
       "      <td>44871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>50.316402</td>\n",
       "      <td>54.336334</td>\n",
       "      <td>50.062355</td>\n",
       "      <td>53.952770</td>\n",
       "      <td>53.952770</td>\n",
       "      <td>22942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>55.168217</td>\n",
       "      <td>56.528118</td>\n",
       "      <td>54.321388</td>\n",
       "      <td>54.495735</td>\n",
       "      <td>54.495735</td>\n",
       "      <td>18342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>55.412300</td>\n",
       "      <td>55.591629</td>\n",
       "      <td>51.591621</td>\n",
       "      <td>52.239193</td>\n",
       "      <td>52.239193</td>\n",
       "      <td>15319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>52.284027</td>\n",
       "      <td>53.798351</td>\n",
       "      <td>51.746044</td>\n",
       "      <td>52.802086</td>\n",
       "      <td>52.802086</td>\n",
       "      <td>9232100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2004-08-19  49.813286  51.835709  47.800831  49.982655  49.982655  44871300\n",
       "1  2004-08-20  50.316402  54.336334  50.062355  53.952770  53.952770  22942800\n",
       "2  2004-08-23  55.168217  56.528118  54.321388  54.495735  54.495735  18342800\n",
       "3  2004-08-24  55.412300  55.591629  51.591621  52.239193  52.239193  15319700\n",
       "4  2004-08-25  52.284027  53.798351  51.746044  52.802086  52.802086   9232100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing train data\n",
    "\n",
    "dataset_train = pd.read_csv('GOOG.csv')\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb85012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4006, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b3deeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2530320c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8582b4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4006, 7)\n",
      "All Timestamps: 4006\n",
      "Selected Features: ['Open', 'High', 'Low', 'Close', 'Adj Close']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19760\\3968157361.py:8: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dataset_train['Date'] = pd.to_datetime(dataset_train['Date'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# selecting features for training and predictions\n",
    "\n",
    "cols = list(dataset_train)[1:6]\n",
    "\n",
    "# extract dates (will be used in visualization)\n",
    "\n",
    "datelist_train = list(dataset_train['Date'])\n",
    "dataset_train['Date'] = pd.to_datetime(dataset_train['Date'], dayfirst=True)\n",
    "\n",
    "print(f'Training set shape: {dataset_train.shape}')\n",
    "print(f'All Timestamps: {len(datelist_train)}')\n",
    "print(f'Selected Features: {cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e5c74",
   "metadata": {},
   "source": [
    "#### 3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6888109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set : (4006, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  49.813286,   51.835709,   47.800831,   49.982655,   49.982655],\n",
       "       [  50.316402,   54.336334,   50.062355,   53.95277 ,   53.95277 ],\n",
       "       [  55.168217,   56.528118,   54.321388,   54.495735,   54.495735],\n",
       "       ...,\n",
       "       [1523.130005, 1535.329956, 1498.      , 1513.640015, 1513.640015],\n",
       "       [1500.      , 1518.689941, 1486.310059, 1518.      , 1518.      ],\n",
       "       [1521.619995, 1523.439941, 1498.420044, 1515.550049, 1515.550049]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing all the commas and converting data to matrix shape format\n",
    "\n",
    "dataset_train = dataset_train[cols].astype(str)\n",
    "\n",
    "for i in cols:\n",
    "    for j in range(0, len(dataset_train)):\n",
    "        dataset_train[i][j] = dataset_train[i][j].replace(',','')\n",
    "\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# using multiple features (predictors)\n",
    "training_set = dataset_train.to_numpy()\n",
    "\n",
    "print(f'Shape of training set : {training_set.shape}')\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe37fd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.27195197],\n",
       "       [-1.27058974],\n",
       "       [-1.25745309],\n",
       "       ...,\n",
       "       [ 2.71716347],\n",
       "       [ 2.65453724],\n",
       "       [ 2.713075  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:,0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78fe8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3857, 90, 4)\n",
      "y_train shape: (3857, 1)\n"
     ]
    }
   ],
   "source": [
    "# creating a data structure with 90 timestamps and 1 output\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "n_past = 90 #number of past days we need to predict the future\n",
    "n_future = 60 #number of days we need to predict into the future\n",
    "\n",
    "for i in range(n_past,len(training_set_scaled) - n_future + 1):\n",
    "    x_train.append(training_set_scaled[i-n_past:i,0:dataset_train.shape[1]-1])\n",
    "    y_train.append(training_set_scaled[i+n_future-1:i+n_future,0])\n",
    "\n",
    "x_train,y_train = np.array(x_train),np.array(y_train)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac77cc",
   "metadata": {},
   "source": [
    "#### Create a model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe4d4",
   "metadata": {},
   "source": [
    "##### Building the LSTM based Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0acfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages from keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacb86a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model building\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 64,\n",
    "               return_sequences=True,\n",
    "               input_shape = (n_past,dataset_train.shape[1]-1)))\n",
    "model.add(LSTM(units = 10,\n",
    "          return_sequences = False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1,\n",
    "                activation = 'linear'))\n",
    "model.compile(optimizer = Adam(learning_rate=0.01),\n",
    "              loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb88965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.1439\n",
      "Epoch 1: val_loss improved from inf to 0.42877, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - loss: 0.1351 - val_loss: 0.4288 - learning_rate: 0.0100\n",
      "Epoch 2/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.0355\n",
      "Epoch 2: val_loss improved from 0.42877 to 0.29993, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 0.0354 - val_loss: 0.2999 - learning_rate: 0.0100\n",
      "Epoch 3/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0311\n",
      "Epoch 3: val_loss improved from 0.29993 to 0.27919, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - loss: 0.0309 - val_loss: 0.2792 - learning_rate: 0.0100\n",
      "Epoch 4/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 0.0310\n",
      "Epoch 4: val_loss did not improve from 0.27919\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step - loss: 0.0310 - val_loss: 0.4239 - learning_rate: 0.0100\n",
      "Epoch 5/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0278\n",
      "Epoch 5: val_loss improved from 0.27919 to 0.21550, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 219ms/step - loss: 0.0280 - val_loss: 0.2155 - learning_rate: 0.0100\n",
      "Epoch 6/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 0.0321\n",
      "Epoch 6: val_loss improved from 0.21550 to 0.18888, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 250ms/step - loss: 0.0319 - val_loss: 0.1889 - learning_rate: 0.0100\n",
      "Epoch 7/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.0332\n",
      "Epoch 7: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step - loss: 0.0329 - val_loss: 0.2928 - learning_rate: 0.0100\n",
      "Epoch 8/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.0323\n",
      "Epoch 8: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - loss: 0.0322 - val_loss: 0.5433 - learning_rate: 0.0100\n",
      "Epoch 9/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.0287\n",
      "Epoch 9: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - loss: 0.0286 - val_loss: 0.2868 - learning_rate: 0.0100\n",
      "Epoch 10/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.0272\n",
      "Epoch 10: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step - loss: 0.0272 - val_loss: 0.3085 - learning_rate: 0.0100\n",
      "Epoch 11/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 0.0255\n",
      "Epoch 11: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - loss: 0.0257 - val_loss: 0.2333 - learning_rate: 0.0100\n",
      "Epoch 12/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 0.0256\n",
      "Epoch 12: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - loss: 0.0257 - val_loss: 0.2329 - learning_rate: 0.0100\n",
      "Epoch 13/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 0.0250\n",
      "Epoch 13: val_loss did not improve from 0.18888\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 187ms/step - loss: 0.0250 - val_loss: 0.2244 - learning_rate: 0.0100\n",
      "Epoch 14/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 0.0286\n",
      "Epoch 14: val_loss improved from 0.18888 to 0.18529, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - loss: 0.0284 - val_loss: 0.1853 - learning_rate: 0.0100\n",
      "Epoch 15/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 0.0264\n",
      "Epoch 15: val_loss improved from 0.18529 to 0.17659, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 182ms/step - loss: 0.0263 - val_loss: 0.1766 - learning_rate: 0.0100\n",
      "Epoch 16/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.0238\n",
      "Epoch 16: val_loss improved from 0.17659 to 0.16204, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - loss: 0.0239 - val_loss: 0.1620 - learning_rate: 0.0100\n",
      "Epoch 17/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0242\n",
      "Epoch 17: val_loss improved from 0.16204 to 0.14060, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 0.0244 - val_loss: 0.1406 - learning_rate: 0.0100\n",
      "Epoch 18/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0245\n",
      "Epoch 18: val_loss improved from 0.14060 to 0.12912, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - loss: 0.0245 - val_loss: 0.1291 - learning_rate: 0.0100\n",
      "Epoch 19/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0264\n",
      "Epoch 19: val_loss did not improve from 0.12912\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 157ms/step - loss: 0.0262 - val_loss: 0.1628 - learning_rate: 0.0100\n",
      "Epoch 20/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.0259\n",
      "Epoch 20: val_loss did not improve from 0.12912\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - loss: 0.0258 - val_loss: 0.1292 - learning_rate: 0.0100\n",
      "Epoch 21/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0231\n",
      "Epoch 21: val_loss improved from 0.12912 to 0.12735, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - loss: 0.0231 - val_loss: 0.1274 - learning_rate: 0.0100\n",
      "Epoch 22/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.0255\n",
      "Epoch 22: val_loss improved from 0.12735 to 0.12290, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - loss: 0.0254 - val_loss: 0.1229 - learning_rate: 0.0100\n",
      "Epoch 23/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.0244\n",
      "Epoch 23: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 162ms/step - loss: 0.0244 - val_loss: 0.1485 - learning_rate: 0.0100\n",
      "Epoch 24/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0244\n",
      "Epoch 24: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 148ms/step - loss: 0.0244 - val_loss: 0.1391 - learning_rate: 0.0100\n",
      "Epoch 25/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.0244\n",
      "Epoch 25: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - loss: 0.0246 - val_loss: 0.1497 - learning_rate: 0.0100\n",
      "Epoch 26/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0247\n",
      "Epoch 26: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - loss: 0.0248 - val_loss: 0.1791 - learning_rate: 0.0100\n",
      "Epoch 27/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0243\n",
      "Epoch 27: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - loss: 0.0244 - val_loss: 0.1698 - learning_rate: 0.0100\n",
      "Epoch 28/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 0.0241\n",
      "Epoch 28: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - loss: 0.0241 - val_loss: 0.2502 - learning_rate: 0.0100\n",
      "Epoch 29/30\n",
      "\u001b[1m12/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0276\n",
      "Epoch 29: val_loss did not improve from 0.12290\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 150ms/step - loss: 0.0275 - val_loss: 0.1697 - learning_rate: 0.0100\n",
      "Epoch 30/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.0255\n",
      "Epoch 30: val_loss improved from 0.12290 to 0.11288, saving model to weights.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 163ms/step - loss: 0.0255 - val_loss: 0.1129 - learning_rate: 0.0100\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss',min_delta=1e-10,patience=10,verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=10,verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.weights.h5',monitor='val_loss',verbose=1,\n",
    "save_best_only=True,save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(x_train,y_train,shuffle=True,epochs = 30,callbacks=[es,rlr,mcp,tb],\n",
    "validation_split=0.2,verbose =1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2a393",
   "metadata": {},
   "source": [
    "#### Make future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c33448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
